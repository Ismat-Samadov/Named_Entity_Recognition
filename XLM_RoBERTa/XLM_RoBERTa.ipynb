{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4MspzaiZIuGF",
        "outputId": "1ce5b394-2de5-4a97-95ad-cdf12c97a31f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets seqeval huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification, TrainingArguments, Trainer, AutoModelForTokenClassification, get_linear_schedule_with_warmup, EarlyStoppingCallback\n",
        "from torch.optim import AdamW\n",
        "from torch.amp import GradScaler\n",
        "import torch\n",
        "import numpy as np\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "import ast\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMnSUGJNIwvZ",
        "outputId": "5c2fb62a-8fd1-4929-87f5-d6fa7a2b6160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"LocalDoc/azerbaijani-ner-dataset\")\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHYmHer-IwyI",
        "outputId": "b888e27c-03d1-40c9-b420-00037cacfa55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['index', 'tokens', 'ner_tags'],\n",
            "        num_rows: 99545\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "def preprocess_example(example):\n",
        "    try:\n",
        "        # Convert string representations of lists into actual lists\n",
        "        example[\"tokens\"] = ast.literal_eval(example[\"tokens\"])\n",
        "        example[\"ner_tags\"] = list(map(int, ast.literal_eval(example[\"ner_tags\"])))\n",
        "    except (ValueError, SyntaxError) as e:\n",
        "        # Handle the error: skip or provide default values if data is malformed\n",
        "        print(f\"Skipping malformed example: {example['index']} due to error: {e}\")\n",
        "        example[\"tokens\"] = []\n",
        "        example[\"ner_tags\"] = []\n",
        "    return example\n",
        "\n",
        "# Apply the preprocessing step\n",
        "dataset = dataset.map(preprocess_example)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b9nn1ifyMD37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def tokenize_and_align_labels(example):\n",
        "    # Tokenize the tokens in the single example\n",
        "    tokenized_inputs = tokenizer(\n",
        "        example[\"tokens\"],  # Pass tokens directly\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            labels.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            # Ensure word_idx does not exceed the length of ner_tags\n",
        "            if word_idx < len(example[\"ner_tags\"]):\n",
        "                labels.append(example[\"ner_tags\"][word_idx])\n",
        "            else:\n",
        "                labels.append(-100)\n",
        "        else:\n",
        "            labels.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SESqDjXYKAsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=False)\n",
        "\n",
        "# Create a 90-10 split for training and validation\n",
        "tokenized_datasets = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)\n",
        "\n",
        "print(tokenized_datasets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwNIB3rtLdYb",
        "outputId": "0a313cd9-707e-434f-af2c-54083f9da224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['index', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 89590\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['index', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9955\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = [\n",
        "    \"O\",                   # Outside of any entity\n",
        "    \"B-PERSON\", \"I-PERSON\",           # Person names\n",
        "    \"B-LOCATION\", \"I-LOCATION\",       # Locations\n",
        "    \"B-ORGANISATION\", \"I-ORGANISATION\", # Organizations\n",
        "    \"B-DATE\", \"I-DATE\",               # Dates\n",
        "    \"B-TIME\", \"I-TIME\",               # Times\n",
        "    \"B-MONEY\", \"I-MONEY\",             # Monetary values\n",
        "    \"B-PERCENTAGE\", \"I-PERCENTAGE\",   # Percentages\n",
        "    \"B-FACILITY\", \"I-FACILITY\",       # Facilities\n",
        "    \"B-PRODUCT\", \"I-PRODUCT\",         # Products\n",
        "    \"B-EVENT\", \"I-EVENT\",             # Events\n",
        "    \"B-ART\", \"I-ART\",                 # Artworks\n",
        "    \"B-LAW\", \"I-LAW\",                 # Legal documents\n",
        "    \"B-LANGUAGE\", \"I-LANGUAGE\",       # Languages\n",
        "    \"B-GPE\", \"I-GPE\",                 # Geopolitical entities\n",
        "    \"B-NORP\", \"I-NORP\",               # Nationalities or groups\n",
        "    \"B-ORDINAL\", \"I-ORDINAL\",         # Ordinal numbers\n",
        "    \"B-CARDINAL\", \"I-CARDINAL\",       # Cardinal numbers\n",
        "    \"B-DISEASE\", \"I-DISEASE\",         # Diseases\n",
        "    \"B-CONTACT\", \"I-CONTACT\",         # Contact info\n",
        "    \"B-ADAGE\", \"I-ADAGE\",             # Sayings\n",
        "    \"B-QUANTITY\", \"I-QUANTITY\",       # Quantities\n",
        "    \"B-MISCELLANEOUS\", \"I-MISCELLANEOUS\", # Miscellaneous entities\n",
        "    \"B-POSITION\", \"I-POSITION\",       # Positions\n",
        "    \"B-PROJECT\", \"I-PROJECT\"          # Projects\n",
        "]"
      ],
      "metadata": {
        "id": "qK8GRnh4tdTK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the data collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# Load the model with the correct number of labels\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"xlm-roberta-base\",\n",
        "    num_labels=25,  # Ensure this matches the number of unique labels in `label_list`\n",
        ")\n",
        "\n",
        "# Custom metric function for evaluation\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Map back to the label list, skipping ignored tokens (-100)\n",
        "    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # Compute metrics\n",
        "    precision = precision_score(true_labels, true_predictions)\n",
        "    recall = recall_score(true_labels, true_predictions)\n",
        "    f1 = f1_score(true_labels, true_predictions)\n",
        "\n",
        "    # Optionally print a classification report\n",
        "    print(classification_report(true_labels, true_predictions))\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    }\n",
        "\n",
        "# Define training arguments with optimized parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,                    # Adjusted learning rate for stability\n",
        "    per_device_train_batch_size=64,        # Adjusted for memory optimization\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=8,                    # Adjusted to reduce overfitting\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=4,         # Higher accumulation for smaller batch size\n",
        "    logging_dir='./logs',\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    report_to=\"none\"                       # Disables logging integrations (e.g., WANDB)\n",
        ")\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
        "\n",
        "# Learning rate scheduler setup\n",
        "num_training_steps = len(tokenized_datasets[\"train\"]) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * num_training_steps),  # 10% warmup steps\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# Define the Trainer with early stopping callback\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, lr_scheduler),  # Pass optimizer and scheduler\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Early stopping with patience of 2\n",
        ")\n",
        "\n",
        "training_metrics = trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k3rClRMak_wP",
        "outputId": "e5d22a34-ba3e-4a4d-e6b9-8c1f6a86eb5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2800' max='2800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2800/2800 19:01, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.492728</td>\n",
              "      <td>0.716705</td>\n",
              "      <td>0.536168</td>\n",
              "      <td>0.613429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.437600</td>\n",
              "      <td>0.316428</td>\n",
              "      <td>0.750777</td>\n",
              "      <td>0.673112</td>\n",
              "      <td>0.709826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.329100</td>\n",
              "      <td>0.287577</td>\n",
              "      <td>0.759884</td>\n",
              "      <td>0.705648</td>\n",
              "      <td>0.731763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.329100</td>\n",
              "      <td>0.272844</td>\n",
              "      <td>0.746619</td>\n",
              "      <td>0.727007</td>\n",
              "      <td>0.736683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.276700</td>\n",
              "      <td>0.263447</td>\n",
              "      <td>0.760019</td>\n",
              "      <td>0.727704</td>\n",
              "      <td>0.743511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.253900</td>\n",
              "      <td>0.257133</td>\n",
              "      <td>0.760282</td>\n",
              "      <td>0.732646</td>\n",
              "      <td>0.746208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.253900</td>\n",
              "      <td>0.253817</td>\n",
              "      <td>0.787151</td>\n",
              "      <td>0.716427</td>\n",
              "      <td>0.750126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.238800</td>\n",
              "      <td>0.260643</td>\n",
              "      <td>0.744960</td>\n",
              "      <td>0.737820</td>\n",
              "      <td>0.741373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ART       0.00      0.00      0.00      1882\n",
            "        DATE       0.00      0.00      0.00       835\n",
            "       EVENT       0.00      0.00      0.00        74\n",
            "    FACILITY       0.00      0.00      0.00      1183\n",
            "         LAW       0.00      0.00      0.00      1140\n",
            "    LOCATION       0.66      0.69      0.67      8965\n",
            "       MONEY       0.00      0.00      0.00       552\n",
            "ORGANISATION       0.00      0.00      0.00       498\n",
            "  PERCENTAGE       0.62      0.81      0.70      3652\n",
            "      PERSON       0.89      0.71      0.79      7138\n",
            "     PRODUCT       0.73      0.73      0.73      2598\n",
            "        TIME       0.00      0.00      0.00      1634\n",
            "\n",
            "   micro avg       0.72      0.54      0.61     30151\n",
            "   macro avg       0.24      0.25      0.24     30151\n",
            "weighted avg       0.54      0.54      0.54     30151\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ART       0.52      0.10      0.16      1882\n",
            "        DATE       0.39      0.19      0.26       835\n",
            "       EVENT       0.00      0.00      0.00        74\n",
            "    FACILITY       0.64      0.64      0.64      1183\n",
            "         LAW       0.53      0.52      0.53      1140\n",
            "    LOCATION       0.76      0.77      0.77      8965\n",
            "       MONEY       0.54      0.41      0.46       552\n",
            "ORGANISATION       0.61      0.50      0.55       498\n",
            "  PERCENTAGE       0.72      0.82      0.77      3652\n",
            "      PERSON       0.89      0.78      0.83      7138\n",
            "     PRODUCT       0.77      0.83      0.80      2598\n",
            "        TIME       0.54      0.30      0.38      1634\n",
            "\n",
            "   micro avg       0.75      0.67      0.71     30151\n",
            "   macro avg       0.58      0.49      0.51     30151\n",
            "weighted avg       0.73      0.67      0.69     30151\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ART       0.64      0.11      0.19      1882\n",
            "        DATE       0.49      0.37      0.42       835\n",
            "       EVENT       0.00      0.00      0.00        74\n",
            "    FACILITY       0.64      0.69      0.66      1183\n",
            "         LAW       0.62      0.50      0.56      1140\n",
            "    LOCATION       0.78      0.79      0.79      8965\n",
            "       MONEY       0.57      0.47      0.52       552\n",
            "ORGANISATION       0.63      0.65      0.64       498\n",
            "  PERCENTAGE       0.76      0.81      0.79      3652\n",
            "      PERSON       0.87      0.81      0.84      7138\n",
            "     PRODUCT       0.79      0.84      0.82      2598\n",
            "        TIME       0.52      0.46      0.49      1634\n",
            "\n",
            "   micro avg       0.76      0.71      0.73     30151\n",
            "   macro avg       0.61      0.54      0.56     30151\n",
            "weighted avg       0.75      0.71      0.71     30151\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ART       0.61      0.13      0.21      1882\n",
            "        DATE       0.45      0.43      0.44       835\n",
            "       EVENT       0.62      0.07      0.12        74\n",
            "    FACILITY       0.68      0.68      0.68      1183\n",
            "         LAW       0.57      0.61      0.59      1140\n",
            "    LOCATION       0.75      0.83      0.78      8965\n",
            "       MONEY       0.57      0.51      0.54       552\n",
            "ORGANISATION       0.63      0.68      0.66       498\n",
            "  PERCENTAGE       0.75      0.84      0.79      3652\n",
            "      PERSON       0.87      0.81      0.84      7138\n",
            "     PRODUCT       0.81      0.85      0.83      2598\n",
            "        TIME       0.56      0.46      0.51      1634\n",
            "\n",
            "   micro avg       0.75      0.73      0.74     30151\n",
            "   macro avg       0.66      0.57      0.58     30151\n",
            "weighted avg       0.74      0.73      0.72     30151\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ART       0.56      0.15      0.23      1882\n",
            "        DATE       0.52      0.40      0.45       835\n",
            "       EVENT       0.81      0.23      0.36        74\n",
            "    FACILITY       0.68      0.70      0.69      1183\n",
            "         LAW       0.59      0.61      0.60      1140\n",
            "    LOCATION       0.78      0.81      0.79      8965\n",
            "       MONEY       0.59      0.53      0.56       552\n",
            "ORGANISATION       0.66      0.69      0.68       498\n",
            "  PERCENTAGE       0.77      0.82      0.79      3652\n",
            "      PERSON       0.87      0.82      0.84      7138\n",
            "     PRODUCT       0.80      0.85      0.83      2598\n",
            "        TIME       0.54      0.51      0.52      1634\n",
            "\n",
            "   micro avg       0.76      0.73      0.74     30151\n",
            "   macro avg       0.68      0.59      0.61     30151\n",
            "weighted avg       0.75      0.73      0.73     30151\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ART       0.49      0.14      0.21      1882\n",
            "        DATE       0.51      0.42      0.46       835\n",
            "       EVENT       0.68      0.36      0.47        74\n",
            "    FACILITY       0.70      0.70      0.70      1183\n",
            "         LAW       0.59      0.63      0.61      1140\n",
            "    LOCATION       0.78      0.81      0.80      8965\n",
            "       MONEY       0.55      0.58      0.56       552\n",
            "ORGANISATION       0.65      0.67      0.66       498\n",
            "  PERCENTAGE       0.76      0.84      0.80      3652\n",
            "      PERSON       0.88      0.82      0.85      7138\n",
            "     PRODUCT       0.80      0.86      0.83      2598\n",
            "        TIME       0.55      0.54      0.54      1634\n",
            "\n",
            "   micro avg       0.76      0.73      0.75     30151\n",
            "   macro avg       0.66      0.61      0.62     30151\n",
            "weighted avg       0.75      0.73      0.73     30151\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ART       0.47      0.16      0.24      1882\n",
            "        DATE       0.57      0.37      0.45       835\n",
            "       EVENT       0.70      0.35      0.47        74\n",
            "    FACILITY       0.73      0.67      0.70      1183\n",
            "         LAW       0.62      0.57      0.60      1140\n",
            "    LOCATION       0.81      0.79      0.80      8965\n",
            "       MONEY       0.57      0.54      0.56       552\n",
            "ORGANISATION       0.69      0.68      0.68       498\n",
            "  PERCENTAGE       0.79      0.81      0.80      3652\n",
            "      PERSON       0.89      0.81      0.85      7138\n",
            "     PRODUCT       0.82      0.85      0.83      2598\n",
            "        TIME       0.61      0.46      0.53      1634\n",
            "\n",
            "   micro avg       0.79      0.72      0.75     30151\n",
            "   macro avg       0.69      0.59      0.63     30151\n",
            "weighted avg       0.77      0.72      0.74     30151\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ART       0.27      0.21      0.24      1882\n",
            "        DATE       0.51      0.46      0.48       835\n",
            "       EVENT       0.69      0.36      0.48        74\n",
            "    FACILITY       0.70      0.70      0.70      1183\n",
            "         LAW       0.60      0.62      0.61      1140\n",
            "    LOCATION       0.78      0.82      0.80      8965\n",
            "       MONEY       0.58      0.58      0.58       552\n",
            "ORGANISATION       0.65      0.69      0.67       498\n",
            "  PERCENTAGE       0.78      0.82      0.80      3652\n",
            "      PERSON       0.87      0.82      0.85      7138\n",
            "     PRODUCT       0.83      0.84      0.83      2598\n",
            "        TIME       0.58      0.50      0.53      1634\n",
            "\n",
            "   micro avg       0.74      0.74      0.74     30151\n",
            "   macro avg       0.65      0.62      0.63     30151\n",
            "weighted avg       0.74      0.74      0.74     30151\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "ryolUcwNJTd_",
        "outputId": "76e84e38-7c41-4fa9-b15e-1231c46a6dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='312' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [156/156 02:38]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ART       0.47      0.16      0.24      1882\n",
            "        DATE       0.57      0.37      0.45       835\n",
            "       EVENT       0.70      0.35      0.47        74\n",
            "    FACILITY       0.73      0.67      0.70      1183\n",
            "         LAW       0.62      0.57      0.60      1140\n",
            "    LOCATION       0.81      0.79      0.80      8965\n",
            "       MONEY       0.57      0.54      0.56       552\n",
            "ORGANISATION       0.69      0.68      0.68       498\n",
            "  PERCENTAGE       0.79      0.81      0.80      3652\n",
            "      PERSON       0.89      0.81      0.85      7138\n",
            "     PRODUCT       0.82      0.85      0.83      2598\n",
            "        TIME       0.61      0.46      0.53      1634\n",
            "\n",
            "   micro avg       0.79      0.72      0.75     30151\n",
            "   macro avg       0.69      0.59      0.63     30151\n",
            "weighted avg       0.77      0.72      0.74     30151\n",
            "\n",
            "{'eval_loss': 0.25381746888160706, 'eval_precision': 0.7871510822826324, 'eval_recall': 0.7164273158435873, 'eval_f1': 0.7501258833538799, 'eval_runtime': 10.7612, 'eval_samples_per_second': 925.08, 'eval_steps_per_second': 14.496, 'epoch': 8.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a path to save the model\n",
        "save_directory = \"./XLM-RoBERTa\"\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(save_directory)\n"
      ],
      "metadata": {
        "id": "HDf12aX-ieEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0e9c93-4bde-449c-827e-db736bffe528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./XLM-RoBERTa/tokenizer_config.json',\n",
              " './XLM-RoBERTa/special_tokens_map.json',\n",
              " './XLM-RoBERTa/sentencepiece.bpe.model',\n",
              " './XLM-RoBERTa/added_tokens.json',\n",
              " './XLM-RoBERTa/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./XLM-RoBERTa\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"./XLM-RoBERTa\")\n",
        "\n",
        "# Initialize the NER pipeline with GPU if available\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)\n",
        "\n",
        "# Define label mapping based on the label list\n",
        "label_mapping = {f\"LABEL_{i}\": label for i, label in enumerate(label_list) if label != \"O\"}\n",
        "\n",
        "# Define the function to prepare test data and evaluate\n",
        "def evaluate_model(test_texts, true_labels):\n",
        "    predictions = []\n",
        "    for text in test_texts:\n",
        "        pred_entities = nlp_ner(text)\n",
        "\n",
        "        # Map predicted labels using the label_mapping and filter out non-entities\n",
        "        pred_labels = [label_mapping.get(entity[\"entity_group\"], \"O\") for entity in pred_entities if entity[\"entity_group\"] in label_mapping]\n",
        "        predictions.append(pred_labels)\n",
        "\n",
        "    # Calculate metrics if lengths match, otherwise notify about inconsistency\n",
        "    if len(true_labels) == len(predictions):\n",
        "        precision = precision_score(true_labels, predictions)\n",
        "        recall = recall_score(true_labels, predictions)\n",
        "        f1 = f1_score(true_labels, predictions)\n",
        "\n",
        "        print(\"Precision:\", precision)\n",
        "        print(\"Recall:\", recall)\n",
        "        print(\"F1-Score:\", f1)\n",
        "        print(classification_report(true_labels, predictions))\n",
        "    else:\n",
        "        print(\"Warning: Inconsistent number of samples between true labels and predictions.\")\n",
        "        print(f\"True Labels: {true_labels}\")\n",
        "        print(f\"Predictions: {predictions}\")\n",
        "\n",
        "# Example test data\n",
        "test_texts = [\"Shahla Khuduyeva və Pasha Sığorta şirkəti haqqında məlumat.\"]\n",
        "true_labels = [[\"B-PERSON\", \"B-ORGANISATION\"]]  # True labels for the sample text\n",
        "evaluate_model(test_texts, true_labels)\n"
      ],
      "metadata": {
        "id": "V70xpZK7Jef7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9661043-f57e-43c1-e2dc-d32ca9083bd9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.5\n",
            "Recall: 0.5\n",
            "F1-Score: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    LOCATION       0.00      0.00      0.00         0\n",
            "ORGANISATION       0.00      0.00      0.00         1\n",
            "      PERSON       1.00      1.00      1.00         1\n",
            "\n",
            "   micro avg       0.50      0.50      0.50         2\n",
            "   macro avg       0.33      0.33      0.33         2\n",
            "weighted avg       0.50      0.50      0.50         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./XLM-RoBERTa\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"./XLM-RoBERTa\")\n",
        "\n",
        "# Initialize the NER pipeline with GPU if available\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)\n",
        "\n",
        "# Define label mapping based on the label list\n",
        "label_list = [\n",
        "    \"O\", \"B-PERSON\", \"I-PERSON\", \"B-LOCATION\", \"I-LOCATION\",\n",
        "    \"B-ORGANISATION\", \"I-ORGANISATION\", \"B-DATE\", \"I-DATE\",\n",
        "    \"B-TIME\", \"I-TIME\", \"B-MONEY\", \"I-MONEY\", \"B-PERCENTAGE\",\n",
        "    \"I-PERCENTAGE\", \"B-FACILITY\", \"I-FACILITY\", \"B-PRODUCT\",\n",
        "    \"I-PRODUCT\", \"B-EVENT\", \"I-EVENT\", \"B-ART\", \"I-ART\",\n",
        "    \"B-LAW\", \"I-LAW\", \"B-LANGUAGE\", \"I-LANGUAGE\", \"B-GPE\",\n",
        "    \"I-GPE\", \"B-NORP\", \"I-NORP\", \"B-ORDINAL\", \"I-ORDINAL\",\n",
        "    \"B-CARDINAL\", \"I-CARDINAL\", \"B-DISEASE\", \"I-DISEASE\",\n",
        "    \"B-CONTACT\", \"I-CONTACT\", \"B-ADAGE\", \"I-ADAGE\",\n",
        "    \"B-QUANTITY\", \"I-QUANTITY\", \"B-MISCELLANEOUS\", \"I-MISCELLANEOUS\",\n",
        "    \"B-POSITION\", \"I-POSITION\", \"B-PROJECT\", \"I-PROJECT\"\n",
        "]\n",
        "label_mapping = {f\"LABEL_{i}\": label for i, label in enumerate(label_list) if label != \"O\"}\n",
        "\n",
        "# Define the function to prepare test data and evaluate\n",
        "def evaluate_model(test_texts, true_labels):\n",
        "    predictions = []\n",
        "    for i, text in enumerate(test_texts):\n",
        "        pred_entities = nlp_ner(text)\n",
        "\n",
        "        # Map predicted labels using the label_mapping and filter out non-entities\n",
        "        pred_labels = [label_mapping.get(entity[\"entity_group\"], \"O\") for entity in pred_entities if entity[\"entity_group\"] in label_mapping]\n",
        "\n",
        "        # Adjust prediction length to match the true labels\n",
        "        if len(pred_labels) != len(true_labels[i]):\n",
        "            print(f\"Warning: Inconsistent number of entities in sample {i+1}. Adjusting predicted entities.\")\n",
        "            pred_labels = pred_labels[:len(true_labels[i])]\n",
        "\n",
        "        predictions.append(pred_labels)\n",
        "\n",
        "    # Calculate metrics if lengths match, otherwise notify about inconsistency\n",
        "    if all(len(true) == len(pred) for true, pred in zip(true_labels, predictions)):\n",
        "        precision = precision_score(true_labels, predictions)\n",
        "        recall = recall_score(true_labels, predictions)\n",
        "        f1 = f1_score(true_labels, predictions)\n",
        "\n",
        "        print(\"Precision:\", precision)\n",
        "        print(\"Recall:\", recall)\n",
        "        print(\"F1-Score:\", f1)\n",
        "        print(classification_report(true_labels, predictions))\n",
        "    else:\n",
        "        print(\"Error: Could not align all samples correctly for evaluation.\")\n",
        "        print(f\"True Labels: {true_labels}\")\n",
        "        print(f\"Predictions: {predictions}\")\n",
        "\n",
        "# Example test data\n",
        "test_texts = [\n",
        "    \"Shahla Khuduyeva və Pasha Sığorta şirkəti haqqında məlumat.\",\n",
        "    \"İlham Əliyev Bakıda keçirilən konfransda çıxış etdi.\",\n",
        "    \"Azərbaycan Respublikası Müdafiə Nazirliyi yeni layihə təqdim etdi.\",\n",
        "    \"Neftçala şəhərində 10 milyondan çox pul sərf edildi.\",\n",
        "    \"2023-cü il avqust ayında bu qərar elan edildi.\"\n",
        "]\n",
        "true_labels = [\n",
        "    [\"B-PERSON\", \"B-ORGANISATION\"],\n",
        "    [\"B-PERSON\", \"B-LOCATION\"],\n",
        "    [\"B-ORGANISATION\"],\n",
        "    [\"B-LOCATION\", \"B-MONEY\"],\n",
        "    [\"B-DATE\"]\n",
        "]\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(test_texts, true_labels)\n"
      ],
      "metadata": {
        "id": "q6zbrBf5Jecq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2568126c-12ca-4a0f-d919-470db98472e3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Inconsistent number of entities in sample 2. Adjusting predicted entities.\n",
            "Warning: Inconsistent number of entities in sample 3. Adjusting predicted entities.\n",
            "Warning: Inconsistent number of entities in sample 4. Adjusting predicted entities.\n",
            "Warning: Inconsistent number of entities in sample 5. Adjusting predicted entities.\n",
            "Precision: 0.25\n",
            "Recall: 0.25\n",
            "F1-Score: 0.25\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DATE       0.00      0.00      0.00         1\n",
            "    LOCATION       0.00      0.00      0.00         2\n",
            "       MONEY       0.00      0.00      0.00         1\n",
            "ORGANISATION       0.00      0.00      0.00         2\n",
            "  PERCENTAGE       0.00      0.00      0.00         0\n",
            "      PERSON       1.00      1.00      1.00         2\n",
            "     PRODUCT       0.00      0.00      0.00         0\n",
            "\n",
            "   micro avg       0.25      0.25      0.25         8\n",
            "   macro avg       0.14      0.14      0.14         8\n",
            "weighted avg       0.25      0.25      0.25         8\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_67rCZJJeYn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}