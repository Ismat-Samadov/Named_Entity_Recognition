{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0nT8HQoGgWMS","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e271ff8d-36d0-40e0-9866-b8a2d6c81a13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.0.0\n","Uninstalling torch-2.0.0:\n","  Successfully uninstalled torch-2.0.0\n","Found existing installation: torch-xla 2.5.1\n","Uninstalling torch-xla-2.5.1:\n","  Successfully uninstalled torch-xla-2.5.1\n","\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting torch==2.1.0\n","  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.16.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (12.1.105)\n","Collecting triton==2.1.0 (from torch==2.1.0)\n","  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.6.77)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n","Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m218.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m207.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, torch\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.0.0\n","    Uninstalling triton-2.0.0:\n","      Successfully uninstalled triton-2.0.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.18 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-2.1.0 triton-2.1.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["nvfuser","torch"]},"id":"f4187bac7a224fd8b3285b7e216876f7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in links: https://storage.googleapis.com/libtpu-releases/index.html\n","Collecting torch_xla>=2.1.0 (from torch_xla[tpu]>=2.1.0)\n","  Downloading torch_xla-2.5.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (17 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torch_xla>=2.1.0->torch_xla[tpu]>=2.1.0) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_xla>=2.1.0->torch_xla[tpu]>=2.1.0) (1.26.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from torch_xla>=2.1.0->torch_xla[tpu]>=2.1.0) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_xla>=2.1.0->torch_xla[tpu]>=2.1.0) (2.32.3)\n","Requirement already satisfied: libtpu-nightly==0.1.dev20240916 in /usr/local/lib/python3.10/dist-packages (from torch_xla[tpu]>=2.1.0) (0.1.dev20240916+nightly)\n","Requirement already satisfied: tpu-info in /usr/local/lib/python3.10/dist-packages (from torch_xla[tpu]>=2.1.0) (0.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_xla>=2.1.0->torch_xla[tpu]>=2.1.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_xla>=2.1.0->torch_xla[tpu]>=2.1.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_xla>=2.1.0->torch_xla[tpu]>=2.1.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_xla>=2.1.0->torch_xla[tpu]>=2.1.0) (2024.8.30)\n","Requirement already satisfied: grpcio>=1.65.5 in /usr/local/lib/python3.10/dist-packages (from tpu-info->torch_xla[tpu]>=2.1.0) (1.67.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tpu-info->torch_xla[tpu]>=2.1.0) (3.20.3)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from tpu-info->torch_xla[tpu]>=2.1.0) (13.9.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->tpu-info->torch_xla[tpu]>=2.1.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->tpu-info->torch_xla[tpu]>=2.1.0) (2.18.0)\n","Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->tpu-info->torch_xla[tpu]>=2.1.0) (4.12.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->tpu-info->torch_xla[tpu]>=2.1.0) (0.1.2)\n","Downloading torch_xla-2.5.1-cp310-cp310-manylinux_2_28_x86_64.whl (90.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 MB\u001b[0m \u001b[31m209.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hTraceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3070, in _dep_map\n","    return self.__dep_map\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2863, in __getattr__\n","    raise AttributeError(attr)\n","AttributeError: _DistInfoDistribution__dep_map\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n","    conflicts = self._determine_conflicts(to_install)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n","    return check_install_conflicts(to_install)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n","    package_set, _ = create_package_set_from_installed()\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n","    dependencies = list(dist.iter_dependencies())\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 247, in iter_dependencies\n","    return self._dist.requires(extras)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2786, in requires\n","    dm = self._dep_map\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3072, in _dep_map\n","    self.__dep_map = self._compute_dependencies()\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in _compute_dependencies\n","    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in <listcomp>\n","    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3086, in reqs_for_extra\n","    if not req.marker or req.marker.evaluate({'extra': extra}):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 325, in evaluate\n","    return _evaluate_markers(self._markers, current_environment)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 224, in _evaluate_markers\n","    lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 198, in _normalize\n","    return tuple(canonicalize_name(v) for v in values)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n","    return command.main(cmd_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n","    return self._main(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n","    return run(options, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n","    self._log(CRITICAL, msg, args, **kwargs)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n","    self.handle(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n","    self.callHandlers(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n","    hdlr.handle(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n","    self.emit(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 169, in emit\n","    renderable = self.render_message(record, message)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/logging.py\", line 185, in render_message\n","    message_text = Text.from_markup(message) if use_markup else Text(message)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 155, in __init__\n","    sanitized_text = strip_control_codes(text)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/control.py\", line 187, in strip_control_codes\n","    def strip_control_codes(\n","KeyboardInterrupt\n","^C\n","Collecting torchvision==0.16.0\n","  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (2.32.3)\n","Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (2.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (11.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (3.16.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision==0.16.0) (2.1.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->torchvision==0.16.0) (12.6.77)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision==0.16.0) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision==0.16.0) (1.3.0)\n","Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchvision\n"]}],"source":["# Clean up existing installations\n","!pip uninstall -y torch torch_xla torchvision torchaudio\n","\n","# Install PyTorch 2.1.0 and matching XLA version\n","!pip install --no-cache-dir torch==2.1.0\n","!pip install --no-cache-dir \"torch_xla[tpu]>=2.1.0\" -f https://storage.googleapis.com/libtpu-releases/index.html\n","\n","# Install supporting packages\n","!pip install torchvision==0.16.0\n","!pip install -q transformers==4.35.2 datasets==2.14.5 seqeval==1.2.2\n","!pip install -q pandas pyarrow\n","\n","# Configure environment\n","import os\n","os.environ['XLA_USE_BF16'] = \"1\"\n","os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = \"100000000\"\n","\n","print(\"\\nInstallation complete. Please restart the runtime now and run the verification code.\")\n","\n","# Check installation status\n","!python3 -c \"import torch; import torch_xla; print(f'PyTorch version: {torch.__version__}'); print(f'XLA version: {torch_xla.__version__}')\""]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"rI01riIUrLbf","executionInfo":{"status":"error","timestamp":1730404473492,"user_tz":-240,"elapsed":7431,"user":{"displayName":"Ismat Samadov","userId":"13714662825869203427"}},"outputId":"c3677c16-a096-4034-9cec-1ea22fa14c6e"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'torch_xla'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9edd0df60231>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_loader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_multiprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_xla'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import torch\n","import torch_xla\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","\n","def verify_tpu_setup():\n","    try:\n","        print(f\"PyTorch version: {torch.__version__}\")\n","        print(f\"PyTorch XLA version: {torch_xla.__version__}\")\n","\n","        # Initialize TPU device\n","        device = xm.xla_device()\n","        print(f\"XLA device: {device}\")\n","\n","        # Run a simple test\n","        input_tensor = torch.randn(3, 3)\n","        device_tensor = input_tensor.to(device)\n","        result = device_tensor @ device_tensor\n","        xm.mark_step()\n","\n","        print(\"\\nTest computation result:\")\n","        print(result)\n","\n","        print(\"\\nTPU setup successful!\")\n","        return True\n","\n","    except Exception as e:\n","        print(f\"\\nError during TPU setup: {str(e)}\")\n","        print(\"\\nTroubleshooting steps:\")\n","        print(\"1. Verify TPU runtime is selected in Runtime -> Change runtime type\")\n","        print(\"2. Make sure you've restarted the runtime after installation\")\n","        print(\"3. Try the following command to check TPU availability:\")\n","        print(\"   !python -c 'import torch_xla; print(torch_xla.__version__)'\")\n","        return False\n","\n","# Additional TPU system information\n","def print_tpu_info():\n","    print(\"\\nTPU System Information:\")\n","    print(f\"TPU Runtime Version: {os.environ.get('TPU_RUNTIME_VERSION', 'Not available')}\")\n","    print(f\"XRT TPU Config: {os.environ.get('XRT_TPU_CONFIG', 'Not available')}\")\n","    if xm.xrt_world_size() > 1:\n","        print(f\"Number of TPU cores: {xm.xrt_world_size()}\")\n","\n","# Run verification\n","verify_tpu_setup()\n","try:\n","    print_tpu_info()\n","except:\n","    print(\"Could not retrieve additional TPU information\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Znomt9e2pRAX","executionInfo":{"status":"aborted","timestamp":1730404198293,"user_tz":-240,"elapsed":9,"user":{"displayName":"Ismat Samadov","userId":"13714662825869203427"}}},"outputs":[],"source":["\"\"\"\n","Azerbaijani Named Entity Recognition (NER) Pipeline - TPU Version\n","License: CC BY-NC-ND 4.0\n","\"\"\"\n","\n","import os\n","import multiprocessing\n","import logging\n","import warnings\n","import json\n","import ast\n","from datetime import datetime\n","from typing import List, Dict, Tuple, Optional\n","from pathlib import Path\n","import sys\n","import traceback\n","import numpy as np\n","import pandas as pd\n","\n","# TPU-specific imports\n","import torch\n","import torch_xla\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","from torch.utils.data import DataLoader\n","\n","from datasets import Dataset, DatasetDict, Features, Sequence, Value\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForTokenClassification,\n","    DataCollatorForTokenClassification,\n","    TrainingArguments,\n","    Trainer,\n","    EarlyStoppingCallback\n",")\n","from seqeval.metrics import f1_score, precision_score, recall_score\n","\n","# Disable wandb\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","# Filter warnings\n","warnings.filterwarnings('ignore', category=FutureWarning)\n","warnings.filterwarnings('ignore', category=UserWarning)\n","\n","# TPU-specific constants\n","TPU_CORES = 8\n","OPTIMAL_NUM_WORKERS = 4  # Adjusted for TPU\n","\n","# Set up logging\n","logging.basicConfig(\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    level=logging.INFO,\n","    handlers=[\n","        logging.StreamHandler(),\n","        logging.FileHandler('training.log')\n","    ]\n",")\n","\n","# Entity type definitions\n","ENTITY_TYPES = {\n","    0: \"O\",           # Outside any named entity\n","    1: \"PERSON\",      # Names of individuals\n","    2: \"LOCATION\",    # Geographical locations\n","    3: \"ORGANISATION\",# Names of companies, institutions\n","    4: \"DATE\",        # Dates or periods\n","    5: \"TIME\",        # Times of the day\n","    6: \"MONEY\",       # Monetary values\n","    7: \"PERCENTAGE\",  # Percentage values\n","    8: \"FACILITY\",    # Buildings, airports, etc.\n","    9: \"PRODUCT\",     # Products and goods\n","    10: \"EVENT\",      # Events and occurrences\n","    11: \"ART\",        # Artworks, titles of books, songs\n","    12: \"LAW\",        # Legal documents\n","    13: \"LANGUAGE\",   # Languages\n","    14: \"GPE\",        # Countries, cities, states\n","    15: \"NORP\",       # Nationalities or religious or political groups\n","    16: \"ORDINAL\",    # Ordinal numbers\n","    17: \"CARDINAL\",   # Cardinal numbers\n","    18: \"DISEASE\",    # Diseases and medical conditions\n","    19: \"CONTACT\",    # Contact information\n","    20: \"ADAGE\",      # Proverbs, sayings\n","    21: \"QUANTITY\",   # Measurements and quantities\n","    22: \"MISCELLANEOUS\", # Miscellaneous entities\n","    23: \"POSITION\",   # Professional or social positions\n","    24: \"PROJECT\"     # Names of projects or programs\n","}\n","\n","def get_training_args(output_dir: str) -> TrainingArguments:\n","    \"\"\"Get TPU-optimized training arguments\"\"\"\n","    return TrainingArguments(\n","        output_dir=output_dir,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=100,\n","        learning_rate=2e-5,\n","        per_device_train_batch_size=32,  # Increased for TPU\n","        per_device_eval_batch_size=32,   # Increased for TPU\n","        num_train_epochs=5,\n","        weight_decay=0.01,\n","        push_to_hub=False,\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"f1\",\n","        logging_dir=os.path.join(output_dir, 'logs'),\n","        logging_steps=50,\n","        report_to=\"none\",\n","        save_strategy=\"steps\",\n","        save_steps=100,\n","        save_total_limit=2,\n","        warmup_steps=500,\n","        fp16=False,  # TPU doesn't need fp16\n","        dataloader_num_workers=OPTIMAL_NUM_WORKERS,\n","        group_by_length=True,\n","        gradient_accumulation_steps=1,  # Adjusted for TPU\n","        max_grad_norm=1.0,\n","        tpu_num_cores=TPU_CORES,\n","        optim=\"adamw_torch\",\n","    )\n","\n","def setup_tpu():\n","    \"\"\"Initialize TPU device\"\"\"\n","    try:\n","        device = xm.xla_device()\n","        logging.info(f\"TPU device initialized: {device}\")\n","        return device\n","    except Exception as e:\n","        logging.error(f\"Failed to initialize TPU: {str(e)}\")\n","        raise\n","\n","\n","class AzerbaijaniNERPipeline:\n","    def __init__(self, model_name: str = \"bert-base-multilingual-cased\", output_dir: str = \"az-ner-model\"):\n","        \"\"\"Initialize the TPU-enabled NER pipeline\"\"\"\n","        self.model_name = model_name\n","        self.output_dir = Path(output_dir)\n","        self.output_dir.mkdir(parents=True, exist_ok=True)\n","\n","        # Initialize tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(\n","            model_name,\n","            model_max_length=512,\n","            padding_side='right'\n","        )\n","\n","        # Initialize label mappings\n","        self.label2id = {entity_type: idx for idx, entity_type in ENTITY_TYPES.items()}\n","        self.id2label = {idx: entity_type for idx, entity_type in ENTITY_TYPES.items()}\n","\n","        # Set up TPU device\n","        self.device = setup_tpu()\n","        logging.info(f\"Using device: {self.device}\")\n","\n","        # Initialize statistics tracking\n","        self.stats = {\n","            'total_examples': 0,\n","            'valid_examples': 0,\n","            'invalid_examples': 0,\n","            'tag_distribution': {idx: 0 for idx in ENTITY_TYPES.keys()}\n","        }\n","\n","    def process_row(self, row: Dict) -> Optional[Dict]:\n","        \"\"\"Process a single data row\"\"\"\n","        try:\n","            self.stats['total_examples'] += 1\n","\n","            tokens = row['tokens']\n","            tags = row['ner_tags']\n","\n","            if isinstance(tokens, str):\n","                tokens = ast.literal_eval(tokens)\n","            if isinstance(tags, str):\n","                tags = ast.literal_eval(tags)\n","\n","            if len(tokens) != len(tags):\n","                self.stats['invalid_examples'] += 1\n","                return None\n","\n","            cleaned_tags = []\n","            for tag in tags:\n","                tag = int(tag) if isinstance(tag, (int, str)) and str(tag).isdigit() else 0\n","                if tag not in ENTITY_TYPES:\n","                    tag = 0\n","                cleaned_tags.append(tag)\n","                self.stats['tag_distribution'][tag] += 1\n","\n","            self.stats['valid_examples'] += 1\n","            return {\n","                'tokens': tokens,\n","                'ner_tags': cleaned_tags\n","            }\n","        except Exception as e:\n","            self.stats['invalid_examples'] += 1\n","            return None\n","\n","    def load_dataset(self, data_path: str) -> DatasetDict:\n","        \"\"\"Load and prepare the dataset with TPU optimization\"\"\"\n","        logging.info(f\"Loading dataset from {data_path}\")\n","\n","        self.stats = {\n","            'total_examples': 0,\n","            'valid_examples': 0,\n","            'invalid_examples': 0,\n","            'tag_distribution': {idx: 0 for idx in ENTITY_TYPES.keys()}\n","        }\n","\n","        try:\n","            df = pd.read_parquet(data_path)\n","            logging.info(f\"Loaded {len(df)} rows from {data_path}\")\n","\n","            # Process rows in parallel for TPU\n","            with multiprocessing.Pool(OPTIMAL_NUM_WORKERS) as pool:\n","                processed_data = list(filter(None, pool.map(self.process_row, df.to_dict('records'))))\n","\n","            dataset = Dataset.from_pandas(\n","                pd.DataFrame(processed_data),\n","                features=Features({\n","                    'tokens': Sequence(Value('string')),\n","                    'ner_tags': Sequence(Value('int64'))\n","                })\n","            )\n","\n","            train_test = dataset.train_test_split(test_size=0.2, seed=42)\n","            test_valid = train_test['test'].train_test_split(test_size=0.5, seed=42)\n","\n","            dataset_dict = DatasetDict({\n","                'train': train_test['train'],\n","                'validation': test_valid['train'],\n","                'test': test_valid['test']\n","            })\n","\n","            self._log_statistics(dataset_dict)\n","            return dataset_dict\n","\n","        except Exception as e:\n","            logging.error(f\"Error loading dataset: {str(e)}\")\n","            raise\n","\n","    def _log_statistics(self, dataset_dict: DatasetDict):\n","        \"\"\"Log dataset statistics\"\"\"\n","        logging.info(\"\\nDataset Statistics:\")\n","        logging.info(f\"Total examples processed: {self.stats['total_examples']}\")\n","        logging.info(f\"Valid examples: {self.stats['valid_examples']}\")\n","        logging.info(f\"Invalid examples: {self.stats['invalid_examples']}\")\n","\n","        logging.info(\"\\nTag Distribution:\")\n","        for tag_id, count in self.stats['tag_distribution'].items():\n","            logging.info(f\"{ENTITY_TYPES[tag_id]}: {count}\")\n","\n","        logging.info(\"\\nDataset Splits:\")\n","        for split, ds in dataset_dict.items():\n","            logging.info(f\"{split} set size: {len(ds)}\")\n","\n","    def tokenize_and_align_labels(self, examples: Dict) -> Dict:\n","        \"\"\"Tokenize and align labels with TPU optimization\"\"\"\n","        tokenized_inputs = self.tokenizer(\n","            examples[\"tokens\"],\n","            truncation=True,\n","            is_split_into_words=True,\n","            max_length=512,  # TPU optimized length\n","            padding=\"max_length\"\n","        )\n","\n","        labels = []\n","        for i, label in enumerate(examples[\"ner_tags\"]):\n","            word_ids = tokenized_inputs.word_ids(batch_index=i)\n","            previous_word_idx = None\n","            label_ids = []\n","\n","            for word_idx in word_ids:\n","                if word_idx is None:\n","                    label_ids.append(-100)\n","                elif word_idx != previous_word_idx:\n","                    label_ids.append(label[word_idx])\n","                else:\n","                    label_ids.append(-100)\n","                previous_word_idx = word_idx\n","\n","            labels.append(label_ids)\n","\n","        tokenized_inputs[\"labels\"] = labels\n","        return tokenized_inputs\n","\n","    def compute_metrics(self, eval_preds: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n","        \"\"\"Compute evaluation metrics with TPU support\"\"\"\n","        predictions, labels = eval_preds\n","        predictions = np.argmax(predictions, axis=2)\n","\n","        true_predictions = [\n","            [ENTITY_TYPES[p] for (p, l) in zip(prediction, label) if l != -100]\n","            for prediction, label in zip(predictions, labels)\n","        ]\n","        true_labels = [\n","            [ENTITY_TYPES[l] for (p, l) in zip(prediction, label) if l != -100]\n","            for prediction, label in zip(predictions, labels)\n","        ]\n","\n","        metrics = {\n","            \"precision\": precision_score(true_labels, true_predictions),\n","            \"recall\": recall_score(true_labels, true_predictions),\n","            \"f1\": f1_score(true_labels, true_predictions)\n","        }\n","\n","        for entity_type in set(ENTITY_TYPES.values()) - {'O'}:\n","            entity_preds = [[p == entity_type for p in pred] for pred in true_predictions]\n","            entity_labels = [[l == entity_type for l in label] for label in true_labels]\n","\n","            try:\n","                metrics[f\"{entity_type}_f1\"] = f1_score(entity_labels, entity_preds)\n","            except:\n","                metrics[f\"{entity_type}_f1\"] = 0.0\n","\n","        # Sync metrics across TPU cores\n","        metrics = {k: xm.mesh_reduce('metrics', v, np.mean) for k, v in metrics.items()}\n","        return metrics\n","\n","    def train(self, dataset_dict: DatasetDict) -> Trainer:\n","        \"\"\"Train the model with TPU optimization\"\"\"\n","        logging.info(\"Initializing model for TPU training...\")\n","\n","        model = AutoModelForTokenClassification.from_pretrained(\n","            self.model_name,\n","            num_labels=len(ENTITY_TYPES),\n","            id2label=self.id2label,\n","            label2id=self.label2id\n","        ).to(self.device)\n","\n","        total_params = sum(p.numel() for p in model.parameters())\n","        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","        logging.info(f\"Model parameters: {total_params:,} (trainable: {trainable_params:,})\")\n","\n","        tokenized_datasets = dataset_dict.map(\n","            self.tokenize_and_align_labels,\n","            batched=True,\n","            remove_columns=dataset_dict[\"train\"].column_names,\n","            num_proc=OPTIMAL_NUM_WORKERS,\n","            load_from_cache_file=False\n","        )\n","\n","        training_args = get_training_args(str(self.output_dir))\n","\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=tokenized_datasets[\"train\"],\n","            eval_dataset=tokenized_datasets[\"validation\"],\n","            tokenizer=self.tokenizer,\n","            data_collator=DataCollatorForTokenClassification(\n","                self.tokenizer,\n","                pad_to_multiple_of=8\n","            ),\n","            compute_metrics=self.compute_metrics,\n","            callbacks=[\n","                EarlyStoppingCallback(\n","                    early_stopping_patience=3,\n","                    early_stopping_threshold=0.01\n","                )\n","            ]\n","        )\n","\n","        try:\n","            # TPU-specific training loop\n","            def train_func(rank):\n","                train_result = trainer.train()\n","                xm.mark_step()\n","                return train_result\n","\n","            train_result = xmp.spawn(train_func, nprocs=TPU_CORES, start_method='fork')\n","            metrics = train_result[0].metrics\n","\n","            trainer.save_metrics(\"train\", metrics)\n","            xm.save(model.state_dict(), str(self.output_dir / \"pytorch_model.bin\"))\n","            self.tokenizer.save_pretrained(str(self.output_dir))\n","\n","            logging.info(f\"Training metrics: {metrics}\")\n","\n","        except Exception as e:\n","            logging.error(f\"Training error: {str(e)}\")\n","            raise\n","\n","        return trainer\n","\n","def main():\n","    \"\"\"Main function to run the TPU pipeline\"\"\"\n","    if not xm.xla_device_hw() == 'TPU':\n","        raise RuntimeError(\"TPU device not found. Please ensure you're running in a TPU runtime.\")\n","\n","    start_time = datetime.now()\n","\n","    try:\n","        pipeline = AzerbaijaniNERPipeline()\n","        data_path = \"train-00000-of-00001.parquet\"\n","        dataset_dict = pipeline.load_dataset(data_path)\n","\n","        def training_loop():\n","            trainer = pipeline.train(dataset_dict)\n","            return trainer\n","\n","        trainer = xmp.spawn(training_loop, nprocs=TPU_CORES, start_method='fork')[0]\n","\n","        def evaluation_loop():\n","            test_results = trainer.evaluate(\n","                dataset_dict[\"test\"].map(\n","                    pipeline.tokenize_and_align_labels,\n","                    batched=True,\n","                    remove_columns=dataset_dict[\"test\"].column_names\n","                )\n","            )\n","            xm.mark_step()\n","            return test_results\n","\n","        test_results = xmp.spawn(evaluation_loop, nprocs=TPU_CORES, start_method='fork')[0]\n","\n","        test_results[\"timestamp\"] = datetime.now().isoformat()\n","        test_results[\"training_duration\"] = str(datetime.now() - start_time)\n","\n","        results_path = pipeline.output_dir / f\"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n","        with open(results_path, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(test_results, f, indent=2, ensure_ascii=False)\n","\n","        logging.info(f\"Training completed successfully.\")\n","        logging.info(f\"Results saved to {results_path}\")\n","        logging.info(f\"Total training time: {datetime.now() - start_time}\")\n","\n","    except Exception as e:\n","        logging.error(f\"Pipeline error: {str(e)}\", exc_info=True)\n","        raise\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AyGakq_PpRDO","executionInfo":{"status":"aborted","timestamp":1730404198293,"user_tz":-240,"elapsed":8,"user":{"displayName":"Ismat Samadov","userId":"13714662825869203427"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sG7_hVSOpRGR","executionInfo":{"status":"aborted","timestamp":1730404198293,"user_tz":-240,"elapsed":8,"user":{"displayName":"Ismat Samadov","userId":"13714662825869203427"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrtyFoJwpRJE","executionInfo":{"status":"aborted","timestamp":1730404198293,"user_tz":-240,"elapsed":8,"user":{"displayName":"Ismat Samadov","userId":"13714662825869203427"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[],"authorship_tag":"ABX9TyOby5VYUz+/4+OFi4SqobuE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}